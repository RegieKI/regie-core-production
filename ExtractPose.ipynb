{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This nodebook extracts poses from a data source\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrained import BodyDetector, Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from constants import emotions\n",
    "\n",
    "processed_dir = Path(\"/media/regieki/data/RegieKI/data/processed4\")\n",
    "device_paths = [f for f in processed_dir.iterdir() if f.is_dir()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = Path(\"/media/regieki/data/RegieKI/data/all_faces2\") \n",
    "\n",
    "\n",
    "detector = BodyDetector()\n",
    "\n",
    "for emotion in emotions:\n",
    "    i = 0\n",
    "    for device_path in device_paths:\n",
    "        device = device_path.stem\n",
    "        print(f\"Starting {device} -> {emotion}\")\n",
    "        image_dir  = Path(device_path, emotion)\n",
    "\n",
    "        for image_path in tqdm(image_dir.glob(\"*.png\")):\n",
    "            output_dir = Path(data_dir, device, emotion)\n",
    "\n",
    "            print(output_dir.resolve())\n",
    "            \n",
    "            if(not output_dir.exists()):\n",
    "                output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            face_path  = Path(output_dir, f\"image_{i:06d}.jpg\")\n",
    "\n",
    "            if( face_path.exists()):\n",
    "                continue\n",
    "\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "            pose = detector.get_pose(img)\n",
    "\n",
    "            for face in faces:                \n",
    "                i += 1\n",
    "                face.save(face_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycoral.adapters import common\n",
    "# from pycoral.adapters import detect\n",
    "# from pycoral.utils.dataset import read_label_file\n",
    "# from pycoral.utils.edgetpu import make_interpreter\n",
    "\n",
    "#import tflite_runtime.interpreter as tflite\n",
    "import pprint\n",
    "import tensorflow.lite as tflite\n",
    "import numpy as np\n",
    "\n",
    "EDGETPU_SHARED_LIB = 'libedgetpu.so.1'\n",
    "POSENET_SHARED_LIB = \"/home/regieki/regieki-core/pretrained/posenet/posenet_decoder.so\"\n",
    "\n",
    "edgetpu_delegate         = tf.lite.experimental.load_delegate(EDGETPU_SHARED_LIB)\n",
    "posenet_decoder_delegate = tf.lite.experimental.load_delegate(POSENET_SHARED_LIB)\n",
    "\n",
    "delegates = [\n",
    "    edgetpu_delegate,     \n",
    "    posenet_decoder_delegate\n",
    "]\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posenet     = \"../pretrained/posenet/posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import collections\n",
    "\n",
    "class KeypointType(enum.IntEnum):\n",
    "  \"\"\"Pose kepoints.\"\"\"\n",
    "  NOSE = 0\n",
    "  LEFT_EYE = 1\n",
    "  RIGHT_EYE = 2\n",
    "  LEFT_EAR = 3\n",
    "  RIGHT_EAR = 4\n",
    "  LEFT_SHOULDER = 5\n",
    "  RIGHT_SHOULDER = 6\n",
    "  LEFT_ELBOW = 7\n",
    "  RIGHT_ELBOW = 8\n",
    "  LEFT_WRIST = 9\n",
    "  RIGHT_WRIST = 10\n",
    "  LEFT_HIP = 11\n",
    "  RIGHT_HIP = 12\n",
    "  LEFT_KNEE = 13\n",
    "  RIGHT_KNEE = 14\n",
    "  LEFT_ANKLE = 15\n",
    "  RIGHT_ANKLE = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# interpreter = make_interpreter(resnet)\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "interpreter =  tflite.Interpreter(\n",
    "    model_path=posenet,\n",
    "    experimental_delegates=delegates)\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "num_keypoints = int(KeypointType.RIGHT_ANKLE + 1)\n",
    "\n",
    "Pose = collections.namedtuple('Pose', ['keypoints', 'score'])\n",
    "\n",
    "size = (641, 481)\n",
    "\n",
    "image_path = \"../tmp/test.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\").resize(size)\n",
    "input_img = np.expand_dims(np.asarray(image), axis=0)\n",
    "\n",
    "for model in test_models:\n",
    "    print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"inference %s\" % output())\n",
    "\n",
    "    print(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pose in poses:\n",
    "    print(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_landmarks_pb = \"/media/meredityman/DATA/Projects/RegieKi/Code/Production/regieki-core/pretrained/face_landmark_shufflenetv2_1.0-20201204T150037Z-001/face_landmark_shufflenetv2_1.0/keypoints/\"\n",
    "\n",
    "# face_landmarks_tflite = \"/media/meredityman/DATA/Projects/RegieKi/Code/Production/regieki-core/pretrained/face_landmark_shufflenetv2_1.0-20201204T150037Z-001/face_landmark_shufflenetv2_1.0/keypoints.tflite\"\n",
    "\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(face_landmarks_pb)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# tflite_quant_model = converter.convert()\n",
    "\n",
    "\n",
    "# # Save the model.\n",
    "# with open(face_landmarks_tflite, 'wb') as f:\n",
    "#   f.write(tflite_quant_model)\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "interpreter =  tflite.Interpreter(\n",
    "    model_path=face_landmarks,\n",
    "    experimental_delegates=delegates)\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "# for info in interpreter.get_tensor_details():\n",
    "#     print(f\"{info['name']}\")\n",
    "\n",
    "input = interpreter.tensor(interpreter.get_input_details()[0][\"index\"])\n",
    "output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "\n",
    "print(interpreter.get_input_details()[0][\"index\"])\n",
    "print(interpreter.get_output_details()[0][\"index\"])\n",
    "\n",
    "for i in range(10):\n",
    "    input().fill(0.5)\n",
    "    interpreter.invoke()\n",
    "    print(\"inference %s\" % output())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}